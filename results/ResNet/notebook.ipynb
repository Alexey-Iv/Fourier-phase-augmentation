{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"'''\nContains all modules for getting a APFA model.\n'''\nimport torch\nfrom torch import nn\nimport torchvision.models as models\nimport random\nimport torch.nn.functional as F\n\n\nclass Masked_Residual_Aggregation(nn.Module):\n    def __init__(self, in_channels : int, num_parts : int):\n        '''\n            in_channels : a int number, which implies the size of the input tensor to this module.\n            num_parts : a int number, which implies the count of hidden layers. They enhanced the layer's features\n        '''\n        super(Masked_Residual_Aggregation, self).__init__()\n        self.in_channels = in_channels\n        self.num_parts = num_parts\n\n        # Convolutional 1x1 to get a m Layers(attantion layers)\n        self.mask_conv = nn.Conv2d(in_channels, num_parts, kernel_size=1)\n\n        # Convolutional 1x1 to obtain the final h(x) (H x W x (CxM) --> H x W x (C))\n        self.final_mask_conv = nn.Conv2d(in_channels * num_parts, in_channels, kernel_size=1)\n\n    def forward(self, x):\n        masks = torch.sigmoid(self.mask_conv(x)) # --> H x W x m\n\n        # Expand masks(N_layer) to corresponding size of f(x)\n        masks = masks.unsqueeze(2)   # ---> H x W x 1 x m\n\n        # B x C x H x W\n        x_exp = x.unsqueeze(1)\n        attention_layers = x_exp * masks\n        h = attention_layers + x_exp    # --> H x W x C x M\n\n        h = torch.cat([h[:, i, :, :, :] for i in range(self.num_parts)], dim=1)\n\n        h_1 = self.final_mask_conv(h)\n\n        h_output = torch.fft.fft2(h_1, norm='ortho')\n\n        return h_output\n\n\nclass Phase_Based_Augmentation(nn.Module):\n    def __init__(self, gamma=0.1):\n        '''\n            gamma : a int number, which explains adding residual to the phase spectrum of other images in batch.\n        '''\n        super().__init__()\n        self.gamma = gamma\n\n    def forward(self, h_freq):\n        batch_size, channels, H, W = h_freq.shape\n\n        amplitude = torch.abs(h_freq)\n        phase_origin = torch.angle(h_freq)\n\n        h_ran = h_freq[random.randint(0, batch_size-1)] # [channels, H, W]\n\n        phase_random = torch.angle(h_ran)\n\n        phase_new = self.gamma * phase_random.unsqueeze(0) + (1 - self.gamma) * phase_origin\n\n        h_freq_new = amplitude * torch.exp(1j * phase_new)\n\n        return h_freq_new\n\n\nclass Hybrid_Module(nn.Module):\n    # in_channels = кол-во каналов из batch из CNN(HxWxC)\n    # num_parts кол-во масок в Masked Residual Aggregation, гипер параматр\n    # gamma - для фазовой аугментации\n    def __init__(self, in_channels, num_parts, gamma, ifft=True):\n        super().__init__()\n        self.masked_module = Masked_Residual_Aggregation(in_channels, num_parts)\n        self.phase_augm = Phase_Based_Augmentation(gamma)\n        self.ifft = ifft\n\n    def forward(self, x):\n        x = self.masked_module(x)\n\n        x = self.phase_augm(x)\n\n        if self.ifft:\n            x = torch.fft.ifft2(x, norm='ortho').real\n\n        return x\n\n\nclass APFA(nn.Module):\n    def __init__(self, in_channels=2048, num_parts=4, gamma=0.1, num_classes=1000):\n        super().__init__()\n\n        resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n        self.resnet_encoder = nn.Sequential(\n            resnet.conv1,\n            resnet.bn1,\n            resnet.relu,\n            resnet.maxpool,\n            resnet.layer1,\n            resnet.layer2,\n            resnet.layer3,\n            resnet.layer4\n        )\n\n        self.hyb_module = Hybrid_Module(in_channels, num_parts, gamma)\n\n        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.1),\n            nn.Linear(in_channels, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.resnet_encoder(x)  # [B, 2048, H, W]\n\n        x = self.hyb_module(x)\n\n        x = self.pool(x)\n        x = x.flatten(1)\n        x = self.classifier(x)\n\n        return x\n\n\nclass Triplet_Network(nn.Module):\n    def __init__(self, original_model_parameters):\n        super().__init__()\n\n        self.orig_branch = APFA(**original_model_parameters)\n\n    def forward(self, orig_img):\n        norm_emb = F.normalize(self.orig_branch(orig_img), p=2, dim=1)\n\n        return norm_emb\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T15:53:29.550725Z","iopub.execute_input":"2025-05-12T15:53:29.551063Z","iopub.status.idle":"2025-05-12T15:53:29.564626Z","shell.execute_reply.started":"2025-05-12T15:53:29.551033Z","shell.execute_reply":"2025-05-12T15:53:29.563892Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from torchvision.transforms import transforms\nimport torch\nfrom collections import defaultdict\nimport glob\nimport os\nfrom PIL import Image\nimport random\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.nn.functional as F\nfrom sklearn.neighbors import KNeighborsClassifier\nimport numpy as np\n\n\ndef coll_fn_augm(batch):\n    batch_size = len(batch)\n    images, labels = zip(*batch)\n    batch = torch.tensor(images)\n\n    alpha = 0.1\n\n    h_freq = torch.fft.fftshift(torch.fft.fft2(batch, norm='ortho'))\n    amplitude = torch.abs(h_freq)\n\n    phase_origin = torch.angle(h_freq)\n\n    h_ran = h_freq[random.randint(0, batch_size-1)] # [channels, H, W]\n    phase_random = torch.angle(h_ran)\n    phase_new = alpha * phase_random.unsqueeze(0) + (1 - alpha) * phase_origin\n    h_freq_new = amplitude * torch.exp(1j * phase_new)\n\n    output = torch.fft.ifft2(torch.fft.ifftshift(h_freq_new), norm='ortho').real\n\n    return output\n\n\n#(64, 512, 3)\nnorm_transform = transforms.Compose([\n    transforms.Resize((64, 512)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\nanother_transform = transforms.Compose([\n    # Геометрические преобразования (применяются к исходному изображению до изменения размера)\n    transforms.RandomApply([\n        transforms.RandomRotation(10),  # Случайный поворот ±10 градусов\n        transforms.RandomPerspective(  # Перспективные искажения\n            distortion_scale=0.15,\n            p=0.3\n        ),\n    ], p=0.5),\n\n    # Цветовые преобразования (важно для разных условий освещения)\n    transforms.ColorJitter(\n        brightness=0.15,  # Яркость\n        contrast=0.15,    # Контраст\n        saturation=0.1,   # Насыщенность\n        hue=0.05          # Оттенок (малое значение для сохранения цветов радужки)\n    ),\n\n    # Размытия и шумы\n    transforms.RandomApply([\n        transforms.GaussianBlur(  # Гауссово размытие\n            kernel_size=3,\n            sigma=(0.1, 1.5))\n    ], p=0.3),\n\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n])\n\n\n# TODO\n# написать датасет для разбиения обучающей и тестовой выборки\nclass Iris_Classification_Dataset(torch.utils.data.Dataset):\n    def __init__():\n        pass\n\n    def __len__():\n        pass\n\n    def __getitem__():\n        pass\n\n\n\nclass IrisDataset(torch.utils.data.Dataset):\n    def __init__(\n        self,\n        root=None,\n        num_seen_classes=20,\n        transform=None,\n        mode=None,\n        list_files=None\n    ):\n        self.root = root\n        self.transform = transform\n        self.mode = mode\n\n        self.class_to_idxs = defaultdict(list)\n        self.data = []\n\n        all_patients = sorted(\n            glob.glob(os.path.join(self.root, '*')),\n            key=lambda x: int(os.path.basename(x))\n        )\n\n        if mode == \"train\":\n            self.patients = all_patients[:num_seen_classes]\n            self.need_classes = list(range(num_seen_classes * 2))  # Удваиваем количество классов\n\n            for patient_dir in self.patients:\n                patient_id = int(os.path.basename(patient_dir))\n                for eye_dir in ['L', 'R']:\n                    eye_path = os.path.join(patient_dir, eye_dir)\n                    if os.path.exists(eye_path):\n                        images = glob.glob(os.path.join(eye_path, '*.*'))\n                        # Создаем уникальный класс для каждого глаза: L = 2*id, R = 2*id + 1\n                        eye_class = 2 * patient_id if eye_dir == 'L' else 2 * patient_id + 1\n                        self.data.extend([(img, eye_class) for img in images])\n\n        elif mode in [\"test_few\", \"test_all\"]:\n            self.patients = all_patients[num_seen_classes:]\n            # Для теста берем оставшиеся классы, умноженные на 2\n            total_patients = len(all_patients)\n            self.need_classes = [2*i for i in range(num_seen_classes, total_patients)]\n\n            for patient_dir in self.patients:\n                patient_id = int(os.path.basename(patient_dir))\n                for eye_dir in ['L', 'R']:\n                    eye_path = os.path.join(patient_dir, eye_dir)\n                    if os.path.exists(eye_path):\n                        images = glob.glob(os.path.join(eye_path, '*.*'))\n                        # Аналогично train: разделяем классы\n                        eye_class = 2 * patient_id if eye_dir == 'L' else 2 * patient_id + 1\n                        \n                        if mode == \"test_few\":\n                            selected_images = images[:1]\n                        else:\n                            selected_images = images[1:]\n                            \n                        self.data.extend([(img, eye_class) for img in selected_images])\n\n        else:\n            raise ValueError(\"Invalid mode. Use 'train' or 'test'\")\n\n        # Обновляем индексы классов\n        for idx, (_, label) in enumerate(self.data):\n            self.class_to_idxs[label].append(idx)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        img_path, label = self.data[index]\n        image = Image.open(img_path).convert('RGB')\n        \n        if self.transform:\n            image = self.transform(image)\n        #if label > 249:\n        #    print(label)\n        return image, label\n\n\n# class IrisDataset(torch.utils.data.Dataset):\n#     def __init__(\n#         self,\n#         root=None,\n#         num_seen_classes=20,\n#         transform=None,\n#         mode=None,\n#         list_files=None\n#     ):\n#         self.root = root\n#         self.transform = transform\n#         self.mode = mode\n\n#         self.need_classes = []\n#         self.class_to_idxs = defaultdict(list)\n\n#         all_patients = sorted(\n#             glob.glob(os.path.join(self.root, '*')),\n#             key=lambda x: int(os.path.basename(x))\n#         )\n\n        # self.data = []\n        # if mode == \"train\":\n        #     self.patients = all_patients[:num_seen_classes]\n        #     self.need_classes = list(range(num_seen_classes))\n\n        #     for patient_dir in self.patients:\n        #         patient_id = int(os.path.basename(patient_dir))\n        #         for eye_dir in ['L', 'R']:\n        #             eye_path = os.path.join(patient_dir, eye_dir)\n        #             if os.path.exists(eye_path):\n        #                 images = glob.glob(os.path.join(eye_path, '*.*'))\n        #                 self.data.extend([(img, patient_id) for img in images])\n\n    #     elif mode in [\"test_few\", \"test_all\"]:\n    #         self.patients = all_patients[num_seen_classes:]\n    #         self.need_classes = [i for i in range(len(all_patients) - num_seen_classes)]\n\n    #         for patient_dir in self.patients:\n    #             patient_id = int(os.path.basename(patient_dir))\n    #             patient_id -= 1\n\n    #             if patient_id:\n    #                 patient_id += 2\n    #             for eye_dir in ['L', 'R']:\n    #                 if eye_dir == \"R\":\n    #                     patient_id += 1\n\n    #                 eye_path = os.path.join(patient_dir, eye_dir)\n    #                 if os.path.exists(eye_path):\n    #                     images = glob.glob(os.path.join(eye_path, '*.*'))\n    #                     if mode == \"test_few\":\n    #                         selected_images = images[:1]\n    #                     else:\n    #                         selected_images = images[1:]\n    #                     self.data.extend([(img, patient_id) for img in selected_images])\n    #     else:\n    #         raise ValueError(\"Invalid mode. Use 'train' or 'test'\")\n\n    #     for idx, (_, label) in enumerate(self.data):\n    #         self.class_to_idxs[label].append(idx)\n\n    # def __len__(self):\n    #     return len(self.data)\n\n    # def __getitem__(self, index):\n    #     img_path, label = self.data[index]\n    #     image = Image.open(img_path).convert('RGB')\n\n    #     if self.transform:\n    #         image = self.transform(image)\n    #     if label > 249:\n    #         print(f\"label {label}\")\n    #     return image, label\n\n\ndef random_choice_except(options, exception):\n    choice = exception\n    while choice == exception:\n        choice = random.choice(options)\n    return choice\n\n\nclass Triplet(torch.utils.data.Dataset):\n    def __init__(self, dataset):\n        super().__init__()\n        self.dataset = dataset\n\n    def __getitem__(self, index):\n        sample1, target1 = self.dataset[index]\n\n        positive_index = random_choice_except(\n            self.dataset.class_to_idxs[target1],\n            exception=index,\n        )\n        sample2, target2 = self.dataset[positive_index]\n\n        negative_target = random_choice_except(\n            self.dataset.need_classes,\n            exception=target1,\n        )\n\n        negative_index = random.choice(\n            self.dataset.class_to_idxs[negative_target],\n        )\n\n        sample3, target3 = self.dataset[negative_index]\n\n        return [sample1, sample2, sample3], [target1, target2, target3]\n\n    def __len__(self):\n        return len(self.dataset)\n\n\ndef get_dataloaders_to_IRIS(\n        path=None,\n        num_seen=1,\n        batch_size=1,\n        transform_train=None,\n        transform_test=None\n    ):\n\n    if transform_test == None:\n        transform_test = transform_train\n\n    train_data = IrisDataset(\n        path,\n        num_seen,\n        transform_train,\n        \"train\"\n    )\n\n    test_data = IrisDataset(\n        path,\n        num_seen,\n        transform_test,\n        \"test_all\"\n    )\n\n    test_dl = DataLoader(\n        test_data,\n        batch_size,\n        num_workers=4,\n        pin_memory=True,\n    )\n\n    train_dl = DataLoader(\n        train_data,\n        batch_size,\n        num_workers=4,\n        pin_memory=True,\n        shuffle=True,\n    )\n\n    return train_dl, test_dl\n\n\ndef get_dl_2_IRIS(\n    path=None,\n    num_seen=1,\n    batch_size=1,\n    transform_train=None,\n    transform_test=None\n):\n    # if transform_test == None:\n    #     transform_test = transform_train\n\n    train_data = IrisDataset(\n        path,\n        num_seen,\n        transform_train,\n        \"train\"\n    )\n\n    test_data_few = IrisDataset(\n        path,\n        num_seen,\n        transform_train,\n        \"test_few\"\n    )\n\n    test_data_all = IrisDataset(\n        path,\n        num_seen,\n        transform_train,\n        \"test_all\"\n    )\n\n    test_dl_few = DataLoader(\n        test_data_few,\n        batch_size,\n        num_workers=4,\n        pin_memory=True,\n    )\n\n    test_dl_all = DataLoader(\n        test_data_all,\n        batch_size,\n        num_workers=4,\n        pin_memory=True,\n    )\n\n    train_dl = DataLoader(\n        train_data,\n        batch_size,\n        num_workers=4,\n        pin_memory=True,\n        shuffle=True\n    )\n\n    return train_dl, test_dl_few, test_dl_all\n\n\ndef run_emb_net(emb_net, dataloader, device=None, normalize=False):\n    data_x = []\n    data_y = []\n    device = device\n    with torch.no_grad():\n        for inputs, labels in dataloader:\n            feats = emb_net(inputs.to(device))\n            if normalize:\n                feats = F.normalize(feats)\n            feats = feats.detach().cpu().numpy()\n            labels = labels.detach().cpu().numpy()\n            data_x.append(feats)\n            data_y.append(labels)\n\n    data_x = np.concatenate(data_x, axis=0)\n    data_y = np.concatenate(data_y, axis=0)\n    return data_x, data_y\n\n\ndef train_knn(emb_net, oneshot_dl, device, normalize=False):\n    data_x, data_y = run_emb_net(emb_net, oneshot_dl, device, normalize)\n    knn = KNeighborsClassifier(n_neighbors=1)\n    knn = knn.fit(data_x, data_y)\n    return knn\n\n\ndef testing_model(emb_net, test_dl_few, test_dl_all, device=None, normalize=False):\n    data_x, data_y = run_emb_net(emb_net, test_dl_all, device, normalize)\n    knn = train_knn(emb_net, test_dl_few, device)\n\n    total_acc = 0\n    total_cnt = 0\n    for feat, label in zip(data_x, data_y):\n        pred = knn.predict(feat[None]).squeeze(0)\n        total_acc += pred == label\n        total_cnt += 1\n\n    acc = total_acc / total_cnt\n    print(f\"Accuracy = {acc:.2%} ({total_acc} / {total_cnt})\")\n\n    return acc, total_acc, total_cnt\n\n\ndef get_embeddings(model, dataloader, device=None):\n    embeddings = []\n    labels = []\n\n    with torch.no_grad():\n        for images, targets in dataloader:\n            images = images.to(device)\n            outputs = model(images)\n            outputs = F.normalize(outputs)\n            embeddings.append(outputs.cpu().numpy())\n            labels.append(targets.cpu().numpy())\n\n    return np.concatenate(embeddings), np.concatenate(labels)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T15:53:29.565446Z","iopub.execute_input":"2025-05-12T15:53:29.565699Z","iopub.status.idle":"2025-05-12T15:53:30.407031Z","shell.execute_reply.started":"2025-05-12T15:53:29.565673Z","shell.execute_reply":"2025-05-12T15:53:30.406511Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from collections import defaultdict\nimport numpy as np\nfrom scipy.spatial.distance import cdist\nfrom sklearn.metrics import roc_curve\nimport torch\nfrom itertools import combinations\nimport matplotlib.pyplot as plt\n#from model.dataset import get_embeddings\nimport seaborn as sns\nimport matplotlib.patheffects as PathEffects\nimport numpy as np\nimport os\nfrom datetime import datetime\n\n\ndef calculate_eer(embeddings, labels):\n    unique_labels = np.unique(labels)\n    genuine_dists = []\n    impostor_dists = []\n\n    for label in unique_labels:\n        class_indices = np.where(labels == label)[0]\n        class_emb = embeddings[class_indices]\n        # 1. Внутриклассовые сравнения\n        if len(class_indices) >= 2:\n            #class_emb = embeddings[class_indices]\n            dists = cdist(class_emb, class_emb, 'euclidean')\n            genuine = dists[np.triu_indices(len(class_emb), 1)]  # Исправлено\n            genuine_dists.extend(genuine)\n\n        # 2. Межклассовые сравнения\n        other_indices = np.where(labels != label)[0]\n        if len(other_indices) > 0 and len(class_indices) > 0:\n            other_emb = embeddings[other_indices]\n            imp_dists = cdist(class_emb, other_emb, 'euclidean').flatten()\n            impostor_dists.extend(imp_dists)\n\n        # 5. Проверка на пустые списки\n    if len(genuine_dists) == 0 or len(impostor_dists) == 0:\n        return {\n            'eer': 1.0,\n            'threshold': None,\n            'genuine_mean': 0,\n            'impostor_mean': 0,\n            'genuine_std': 0,\n            'impostor_std': 0\n        }\n\n    # 3. Формирование меток\n    y_true = np.concatenate([np.ones(len(genuine_dists)),\n                            np.zeros(len(impostor_dists))])\n    y_score = -np.concatenate([genuine_dists, impostor_dists])\n\n    # 4. Расчет EER\n    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n    fnr = 1 - tpr\n    eer_idxx = np.nanargmin(np.abs(fnr - fpr))\n\n\n    return {\n        'eer': fpr[eer_idxx],\n        'threshold': thresholds[eer_idxx],\n        'genuine_mean': np.mean(genuine_dists),\n        'impostor_mean': np.mean(impostor_dists),\n        'genuine_std': np.std(genuine_dists),\n        'impostor_std': np.std(impostor_dists)\n    }\n\n\n# Новая функция вычисления EER с учетом реального диапазона расстояний\ndef cal_eer(target, imposter):\n    min_score = min(target.min(), imposter.min())\n    max_score = max(target.max(), imposter.max())\n\n    thresholds = torch.linspace(min_score, max_score, 1000)\n\n    fars = torch.tensor([(imposter <= t).float().mean() for t in thresholds])\n    frrs = torch.tensor([(target > t).float().mean() for t in thresholds])\n\n    abs_diffs = torch.abs(fars - frrs)\n    min_index = torch.argmin(abs_diffs)\n\n    eer = (fars[min_index] + frrs[min_index]) / 2\n    eer_threshold = thresholds[min_index]\n\n    return eer, eer_threshold\n\n\nROOT_HIST = None\ndef get_hist(model, test_dl, device=None, out=\"result.png\", subtitle=None, root=None):\n    global ROOT_HIST\n    if root == None:\n        if ROOT_HIST == None:\n            current_time = datetime.now()\n            formated_time = current_time.strftime(\"%m-%d_%H:%M:%S\")\n            ROOT_HIST = f\"hist-{subtitle}-{formated_time}\"\n            root = ROOT_HIST\n            if not os.path.exists(ROOT_HIST):\n                os.makedirs(ROOT_HIST)\n        else:\n            root = ROOT_HIST\n    else:\n        ROOT_HIST = root\n\n    embeddings, labels = get_embeddings(model, test_dl, device)\n\n    # Создаем словарь для группировки\n    class_embeddings = {}\n\n    # Проходим по всем эмбеддингам и меткам\n    for emb, label in zip(embeddings, labels):\n        label = label.item()  # Если метки в тензоре\n\n        if label not in class_embeddings:\n            # Создаем новый ключ с добавлением размерности батча\n            class_embeddings[label] = torch.tensor(emb).unsqueeze(0)\n        else:\n            # Конкатенируем с существующими эмбеддингами класса\n            class_embeddings[label] = torch.cat([\n                class_embeddings[label],\n                torch.tensor(emb).unsqueeze(0)\n            ], dim=0)\n\n    embeddings = class_embeddings\n\n    all_target_scores = []\n    all_imposter_scores = []\n\n    for class_id in embeddings:\n        # Target-пары внутри класса\n        class_embs = embeddings[class_id]\n        if class_embs.shape[0] > 1:\n            # Генерация всех уникальных пар\n            indices = torch.tensor(list(combinations(range(class_embs.shape[0]), 2)))\n            target_pairs_a = class_embs[indices[:, 0]]\n            target_pairs_b = class_embs[indices[:, 1]]\n            target_scores = torch.norm(target_pairs_a - target_pairs_b, p=2, dim=1)\n            all_target_scores.append(target_scores)\n\n        # Imposter-пары с другими классами\n        for other_class_id in embeddings:\n            if other_class_id != class_id:\n                other_embs = embeddings[other_class_id]\n\n                # Генерация всех возможных комбинаций\n                class_indices = torch.arange(class_embs.shape[0])\n                other_indices = torch.arange(other_embs.shape[0])\n\n                # Декартово произведение индексов\n                pairs = torch.cartesian_prod(class_indices, other_indices)\n\n                # Выборка соответствующих эмбеддингов\n                imposter_pairs_a = class_embs[pairs[:, 0]]\n                imposter_pairs_b = other_embs[pairs[:, 1]]\n\n                imposter_scores = torch.norm(imposter_pairs_a - imposter_pairs_b, p=2, dim=1)\n                all_imposter_scores.append(imposter_scores)\n\n    # Объединение результатов\n    target_scores = torch.cat(all_target_scores) if all_target_scores else torch.tensor([])\n    imposter_scores = torch.cat(all_imposter_scores)\n\n\n    # Гистограммы\n\n    # Пересчет и построение графика с новой EER-точкой\n    eer, eer_threshold = cal_eer(target_scores, imposter_scores)\n\n    plt.figure(figsize=(10, 6))\n\n    total_samples_target = target_scores.size()\n    total_samples_imposter = imposter_scores.size()\n\n    # Построение гистограммы с вероятностями\n    plt.hist(target_scores.numpy(), bins=100, alpha=0.5, label='Target',\n            weights=np.ones_like(target_scores.numpy()) / total_samples_target)  # Нормализация\n\n\n    plt.hist(imposter_scores.numpy(), bins=100, alpha=0.5, label='Imposter',\n             weights=np.ones_like(imposter_scores.numpy()) / total_samples_imposter)\n\n    plt.axvline(x=eer_threshold.item(), color='red', linestyle='--', linewidth=2, label=f'EER Threshold ({eer_threshold:.2f})')\n    plt.xlabel('Score (Euclidean)')\n    plt.ylabel('Probability')\n    plt.legend()\n\n    if subtitle != None:\n        plt.suptitle(subtitle)\n\n    plt.savefig(os.path.join(root, out))\n\n    plt.close()\n\n    print(f\"EER: {eer.item():.4f} at threshold: {eer_threshold.item():.4f}\")\n\n\nPATH_PLOT = None\n# Define our own plot function\ndef scatter(x, labels, subtitle=None, root=None):\n    global PATH_PLOT\n    if root == None:\n        if PATH_PLOT == None:\n            current_time = datetime.now()\n            formated_time = current_time.strftime(\"%m-%d_%H:%M:%S\")\n            PATH_PLOT = f\"plot_{subtitle[:-1]}_{formated_time}\"\n            root = PATH_PLOT\n            if not os.path.exists(root):\n                os.makedirs(root)\n        else:\n            root = PATH_PLOT\n    else:\n        PATH_PLOT = root\n\n    unique_labels = np.unique(labels)\n    labels = np.searchsorted(unique_labels, labels)  # Переиндексация в 0,1,2,...\n    num_classes = len(unique_labels)\n    palette = np.array(sns.color_palette(\"hls\", num_classes)) # Choosing color\n    # Create a seaborn scatter plot #\n    f = plt.figure(figsize=(8, 8))\n    ax = plt.subplot(aspect='equal')\n    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40,\n                    c=palette[labels.astype(np.int32)])\n\n    plt.xlim(-25, 25)\n    plt.ylim(-25, 25)\n\n    ax.axis('off')\n    ax.axis('tight')\n\n    # Add label on top of each cluster ##\n    idx2name = [str(x+1) for x in range(num_classes)]\n    txts = []\n    for i in range(num_classes):\n        # Position of each label.\n        xtext, ytext = np.median(x[labels == i, :], axis=0)\n        txt = ax.text(xtext, ytext, idx2name[i], fontsize=24)\n        txt.set_path_effects([\n            PathEffects.Stroke(linewidth=5, foreground=\"w\"),\n            PathEffects.Normal()])\n        txts.append(txt)\n\n\n    if subtitle != None:\n        plt.suptitle(subtitle)\n\n    if not os.path.exists(root):\n        os.makedirs(root)\n\n    plt.savefig(os.path.join(root, str(subtitle)))\n    plt.close()\n\n\ndef zero_shot_inference(sample_embedding, class_embeddings, class_names):\n    \"\"\"\n    Оптимизированная версия с использованием встроенных функций PyTorch\n    \"\"\"\n    max_similarity = -float('inf')\n    predicted_class = None\n\n    for class_name in class_names:\n        # Получаем все эмбеддинги класса (тензор [N, D])\n        class_embs = class_embeddings[class_name]\n\n        # Вычисляем косинусную схожесть сразу для всех примеров класса\n        # Добавляем размерность для батча (из [D] -> [1, D])\n        similarities = F.cosine_similarity(\n            sample_embedding.unsqueeze(0),  # [1, D]\n            class_embs,                    # [N, D]\n            dim=1\n        )\n\n        # Находим максимальную схожесть для класса\n        class_max_sim = torch.max(similarities).item()\n\n        if class_max_sim > max_similarity:\n            max_similarity = class_max_sim\n            predicted_class = class_name\n\n    return predicted_class\n\n\ndef get_emb(model, dataloader, device):\n    \"\"\"Получение эмбеддингов в виде тензоров PyTorch\"\"\"\n    model.eval()\n    embeddings = []\n    labels = []\n    with torch.no_grad():\n        for inputs, targets in dataloader:\n            inputs = inputs.to(device)\n            emb = model(inputs)\n            emb = F.normalize(emb, p=2, dim=1)\n            embeddings.append(emb)  # Сохраняем как тензор\n            labels.append(targets.to(device))\n    return torch.cat(embeddings), torch.cat(labels)\n\n\ndef check(model, model2, test_dl, device):\n    embeddings, labels = get_emb(model, test_dl, device)\n    x, lab = get_emb(model2, test_dl, device)\n\n    # Исправляем преобразование numpy в тензоры\n    class_embeddings = {}\n    for emb, label in zip(embeddings, labels):\n        label = label.item()\n        emb = emb.cpu()  # Переносим на CPU для совместимости\n\n        if label not in class_embeddings:\n            class_embeddings[label] = emb.unsqueeze(0)\n        else:\n            class_embeddings[label] = torch.cat([\n                class_embeddings[label],\n                emb.unsqueeze(0)\n            ], dim=0)\n\n    # Преобразуем все к одному устройству\n    device = next(model.parameters()).device\n    correct = 0\n    for emb, true_label in zip(x, lab):\n        emb = emb.to(device)\n        true_label = true_label.item()\n\n        # Конвертируем эмбеддинги класса к нужному устройству\n        class_embs_on_device = {\n            k: v.to(device) for k, v in class_embeddings.items()\n        }\n\n        predicted_class = zero_shot_inference(\n            emb,\n            class_embs_on_device,\n            list(class_embeddings.keys())\n        )\n        correct += (predicted_class == true_label)\n\n    return correct / len(lab)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T17:05:51.646353Z","iopub.execute_input":"2025-05-12T17:05:51.647132Z","iopub.status.idle":"2025-05-12T17:05:51.678907Z","shell.execute_reply.started":"2025-05-12T17:05:51.647105Z","shell.execute_reply":"2025-05-12T17:05:51.678124Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"import torchvision.models as models\nfrom torch import nn\n\n\ndef get_resnet(num_classes=512):\n    model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n    num_ftrs = model.fc.in_features\n    model.fc = nn.Linear(num_ftrs, num_classes)\n\n    return model\n\ndef get_resnet152(num_classes=512):\n    model = models.resnet152(weights=models.ResNet152_Weights.DEFAULT)\n    num_ftrs = model.fc.in_features\n    model.fc = nn.Linear(num_ftrs, num_classes)\n\n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T15:53:52.353174Z","iopub.execute_input":"2025-05-12T15:53:52.353649Z","iopub.status.idle":"2025-05-12T15:53:52.358670Z","shell.execute_reply.started":"2025-05-12T15:53:52.353626Z","shell.execute_reply":"2025-05-12T15:53:52.357876Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\n\nclass Hard_mining_TripletLoss(nn.Module):\n    def __init__(self, margin=0.5, device=None):\n        super().__init__()\n        self.margin = margin\n        self.device = device\n\n    def _get_anchor_positive_triplet_mask(self, labels):\n        \"\"\"Return a 2D mask where mask[a, p] is True if a and p are distinct and have same label.\n\n        Args:\n            labels: tf.int32 `Tensor` with shape [batch_size]\n\n        Returns:\n            mask: tf.bool `Tensor` with shape [batch_size, batch_size]\n        \"\"\"\n\n        # Check that i and j are distinct\n        indices_equal = torch.eye(labels.size()[0]).bool().to(self.device)\n        indices_not_equal = ~indices_equal # flip booleans\n\n        # Check if labels[i] == labels[j]\n        # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)\n        labels_equal = torch.unsqueeze(labels, 0) == torch.unsqueeze(labels, 1)\n        # Combine the two masks\n        mask = indices_not_equal & labels_equal\n\n        return mask\n\n\n    def _get_anchor_negative_triplet_mask(self, labels):\n        \"\"\"Return a 2D mask where mask[a, n] is True if a and n have distinct labels.\n\n        Args:\n            labels: tf.int32 `Tensor` with shape [batch_size]\n\n        Returns:\n            mask: tf.bool `Tensor` with shape [batch_size, batch_size]\n        \"\"\"\n        # Check if labels[i] != labels[k]\n        # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)\n        labels_equal = torch.unsqueeze(labels, 0) == torch.unsqueeze(labels, 1)\n        mask = ~labels_equal # invert the boolean tensor\n\n        return mask\n\n    def forward(self, embeddings, labels):\n        # Get the pairwise distance matrix\n        pairwise_dist = torch.cdist(embeddings, embeddings, p=2)\n\n        # For each anchor, get the hardest positive\n        # First, we need to get a mask for every valid positive (they should have same label)\n        mask_anchor_positive = self._get_anchor_positive_triplet_mask(labels)\n\n        # We put to 0 any element where (a, p) is not valid (valid if a != p and label(a) == label(p))\n        anchor_positive_dist = mask_anchor_positive * pairwise_dist\n\n        # shape (batch_size, 1)\n        hardest_positive_dist = torch.max(anchor_positive_dist, 1, keepdim=True)[0]\n\n        # For each anchor, get the hardest negative\n        # First, we need to get a mask for every valid negative (they should have different labels)\n        mask_anchor_negative = self._get_anchor_negative_triplet_mask(labels)\n\n        # We add the maximum value in each row to the invalid negatives (label(a) == label(n))\n        max_anchor_negative_dist = torch.max(pairwise_dist, 1, keepdim=True)[0]\n        anchor_negative_dist = pairwise_dist + max_anchor_negative_dist * ~(mask_anchor_negative)\n\n        # shape (batch_size,)\n        hardest_negative_dist = torch.min(anchor_negative_dist, 1, keepdim=True)[0]\n\n        # Combine biggest d(a, p) and smallest d(a, n) into final triplet loss\n        triplet_loss = torch.max(hardest_positive_dist - hardest_negative_dist + self.margin, torch.Tensor([0.0]).to(self.device))\n\n        # Get final mean triplet loss\n        triplet_loss = torch.mean(triplet_loss)\n\n        return triplet_loss\n\n\nclass BatchHardTripletLoss(nn.Module):\n    def __init__(self, margin=0.5, device=None):\n        super().__init__()\n        self.margin = margin\n        self.device = device\n\n    def get_anchor_positive_mask(self, labels):\n        indices_eq = torch.eq(labels.unsqueeze(0), labels.unsqueeze(1))\n        identity = torch.eye(labels.size(0), dtype=torch.bool, device=self.device)\n        return indices_eq & ~identity\n\n    def get_anchor_negative_mask(self, labels):\n        return torch.ne(labels.unsqueeze(0), labels.unsqueeze(1))\n\n    def forward(self, embeddings, labels):\n        pairwise_dist = torch.cdist(embeddings, embeddings, p=2)\n\n        mask_anchor_positive = self.get_anchor_positive_mask(labels)\n        mask_anchor_negative = self.get_anchor_negative_mask(labels)\n\n        # Hardest positive\n        anchor_positive_dist = pairwise_dist * mask_anchor_positive.float()\n        hardest_positive_dist, _ = anchor_positive_dist.max(dim=1, keepdim=True)\n\n        # Hardest negative\n        anchor_negative_dist = pairwise_dist * mask_anchor_negative.float()\n        hardest_negative_dist, _ = anchor_negative_dist.min(dim=1, keepdim=True)\n\n        # Расчет Triplet Loss\n        triplet_loss = F.relu(\n            hardest_positive_dist - hardest_negative_dist + self.margin\n        )\n\n        # Усреднение с учетом валидных триплетов\n        valid_mask = triplet_loss > 1e-16\n        loss = triplet_loss[valid_mask].sum() / (valid_mask.sum().float() + 1e-16)\n\n        return loss if not torch.isnan(loss) else torch.tensor(0.0, device=self.device)\n\n\nclass TripletLoss(nn.Module):\n    def __init__(self, margin, device=None):\n        super().__init__()\n        self.margin = margin\n        self.device = device\n\n    def forward(self, fs, ys):\n        anchor, positive, negative = fs\n        anchor = anchor.to(self.device)\n        positive = positive.to(self.device)\n        negative = negative.to(self.device)\n\n        dist_pos = (anchor - positive).square().sum(axis=-1)\n        dist_neg = (anchor - negative).square().sum(axis=-1)\n\n        loss = F.relu(dist_pos - dist_neg + self.margin)\n\n        return loss.mean()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T15:53:53.462767Z","iopub.execute_input":"2025-05-12T15:53:53.463451Z","iopub.status.idle":"2025-05-12T15:53:53.482704Z","shell.execute_reply.started":"2025-05-12T15:53:53.463418Z","shell.execute_reply":"2025-05-12T15:53:53.482040Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"!gdown --id 1Kc4iP695Ggh_OkDGtIYNzu26jhv58nTA","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T15:53:57.493981Z","iopub.execute_input":"2025-05-12T15:53:57.494504Z","iopub.status.idle":"2025-05-12T15:54:04.555939Z","shell.execute_reply.started":"2025-05-12T15:53:57.494480Z","shell.execute_reply":"2025-05-12T15:54:04.555030Z"}},"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.11/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1Kc4iP695Ggh_OkDGtIYNzu26jhv58nTA\nFrom (redirected): https://drive.google.com/uc?id=1Kc4iP695Ggh_OkDGtIYNzu26jhv58nTA&confirm=t&uuid=254cc0e8-4ebb-4095-914e-79e142174823\nTo: /kaggle/working/norm_photo.zip\n100%|██████████████████████████████████████| 79.9M/79.9M [00:02<00:00, 37.6MB/s]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!unzip -q norm_photo.zip -d /content/data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T15:54:17.713128Z","iopub.execute_input":"2025-05-12T15:54:17.713484Z","iopub.status.idle":"2025-05-12T15:54:18.560101Z","shell.execute_reply.started":"2025-05-12T15:54:17.713430Z","shell.execute_reply":"2025-05-12T15:54:18.559175Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"!ls /content/data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T15:54:20.525975Z","iopub.execute_input":"2025-05-12T15:54:20.526540Z","iopub.status.idle":"2025-05-12T15:54:20.662001Z","shell.execute_reply.started":"2025-05-12T15:54:20.526509Z","shell.execute_reply":"2025-05-12T15:54:20.661294Z"}},"outputs":[{"name":"stdout","text":"Norm_photo\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import torchvision.models as models\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torchvision.datasets import ImageFolder\nimport torchvision\nimport torch.nn.functional as F\nfrom torch.nn.functional import normalize\nfrom torchvision import datasets\nfrom torch import nn\nimport torch.optim as optim\nimport torch\nfrom torchvision.transforms import transforms\nimport numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport random\nimport os\nfrom sklearn.manifold import TSNE\n\n\n## -------------------------------------------------------------- ##\n\n\ntorch.manual_seed(42)\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nNETWORK = False    # Do you want to use new Network? [True] or backbone (resnet) [False]\nSUBTITLE = None\n\n#hyper parameters:\nnum_epochs = 50\nPATH_TO_NORMALIZE_PHOTOES = \"/content/data/Norm_photo\"\nNUM_SEEN_CLASSES = 180\nbatch_size = 64\nmargin = 0.6\nlearning_rate = 0.0001\n\n\noriginal_model_parameters = {\n    'in_channels' : 2048,\n    'num_parts' : 4,\n    'gamma' : 0.05,\n    'num_classes' : 512\n}\n\nPATH_PLOT = None\nROOT_HIST = None\nif __name__ == '__main__':\n\n    if NETWORK:\n        model = Triplet_Network(original_model_parameters).to(device)\n        SUBTITLE = \"Phase Model\"\n    else:\n        model = get_resnet(original_model_parameters['num_classes']).to(device)\n        SUBTITLE = \"ResNet50\"\n\n\n    train_dataloader, few_dataloader, test_dataloader = get_dl_2_IRIS(\n        PATH_TO_NORMALIZE_PHOTOES,\n        NUM_SEEN_CLASSES,\n        batch_size,\n        norm_transform\n    )\n\n    criterion = Hard_mining_TripletLoss(margin=margin, device=device)\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)\n\n    val_losses = []\n\n    best_auc = 0.0\n    train_losses = []\n    val_metrics = {'FAR': [], 'FRR': [], 'ERR': [], 'AUC': []}\n    eer_list = []\n    val_accuracies = []\n    \n    print(f\"Starts fiting the model, Margin = {margin}\")\n    for e in range(num_epochs):\n        model = model.train()\n\n        with tqdm(train_dataloader,\n                desc=f\"Epoch {e+1}/{num_epochs} [Train]\",\n                leave=False,\n                dynamic_ncols=True\n        ) as pbar:\n            train_loss = 0.0\n            for batch in pbar:\n                images, labels = batch\n                images = images.to(device)\n                labels = labels.to(device)\n\n                optimizer.zero_grad()\n\n                embeddings = model(images)\n\n                loss = criterion(embeddings, labels)\n\n                loss.backward()\n\n                optimizer.step()\n\n                train_loss += loss.item() * images.shape[0]\n\n                pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n\n        train_loss /= len(train_dataloader.dataset)\n        train_losses.append(train_loss)\n\n        print(f\"Epoch {e+1}, Loss: {train_loss:.4f} \\n LR: {optimizer.param_groups[0]['lr']}\")\n\n        model.eval()\n        #print(f\"Result : {check(resnet50, model, test_dataloader, device) * 100:.4f} %\")\n        acc, _, __ = testing_model(model, few_dataloader, test_dataloader, device=device)\n        val_accuracies.append(acc)\n        \n        embeddings, labels = get_embeddings(model, test_dataloader, device=device)\n        metrics = calculate_eer(embeddings, labels)\n\n        eer_list.append(metrics['eer'])\n        \n        print(f\"\"\"\n                EER: {metrics['eer']:.7f}\n                Threshold: {metrics['threshold']:.4f}\n                Genuine distances: {metrics['genuine_mean']:.2f} ± {metrics['genuine_std']:.2f}\n                Impostor distances: {metrics['impostor_mean']:.2f} ± {metrics['impostor_std']:.2f}\n        \"\"\")\n\n        if e % 5 == 0:\n            tsne = TSNE(random_state=0, perplexity=20)\n            embeddings, labels = get_embeddings(model, test_dataloader, device=device)\n            train_tsne_embeds = tsne.fit_transform(embeddings)\n\n            get_hist(model, test_dataloader, device, f\"hist_{e}.png\", SUBTITLE)\n\n            scatter(train_tsne_embeds, labels.astype(np.int32), SUBTITLE + str(e))\n\n\n        scheduler.step(train_loss)\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T17:22:22.538331Z","iopub.execute_input":"2025-05-12T17:22:22.538655Z","iopub.status.idle":"2025-05-12T17:35:15.223530Z","shell.execute_reply.started":"2025-05-12T17:22:22.538625Z","shell.execute_reply":"2025-05-12T17:35:15.222523Z"}},"outputs":[{"name":"stdout","text":"Starts fiting the model, Margin = 0.6\n","output_type":"stream"},{"name":"stderr","text":"                                                                                ","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 0.1249 \n LR: 0.0001\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 28.87% (125 / 433)\n\n                EER: 0.2427795\n                Threshold: -0.7323\n                Genuine distances: 0.67 ± 0.10\n                Impostor distances: 0.79 ± 0.09\n        \nEER: 0.2438 at threshold: 0.7329\n","output_type":"stream"},{"name":"stderr","text":"                                                                                ","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Loss: 0.1014 \n LR: 0.0001\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 30.95% (134 / 433)\n\n                EER: 0.2139844\n                Threshold: -0.7266\n                Genuine distances: 0.66 ± 0.10\n                Impostor distances: 0.81 ± 0.11\n        \n","output_type":"stream"},{"name":"stderr","text":"                                                                                ","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Loss: 0.0985 \n LR: 0.0001\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 42.26% (183 / 433)\n\n                EER: 0.1770224\n                Threshold: -0.8939\n                Genuine distances: 0.75 ± 0.16\n                Impostor distances: 1.04 ± 0.16\n        \n","output_type":"stream"},{"name":"stderr","text":"                                                                                ","output_type":"stream"},{"name":"stdout","text":"Epoch 4, Loss: 0.1002 \n LR: 0.0001\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 48.96% (212 / 433)\n\n                EER: 0.1502663\n                Threshold: -0.9860\n                Genuine distances: 0.81 ± 0.17\n                Impostor distances: 1.16 ± 0.17\n        \n","output_type":"stream"},{"name":"stderr","text":"                                                                                ","output_type":"stream"},{"name":"stdout","text":"Epoch 5, Loss: 0.0886 \n LR: 0.0001\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 52.19% (226 / 433)\n\n                EER: 0.1451905\n                Threshold: -1.0252\n                Genuine distances: 0.78 ± 0.21\n                Impostor distances: 1.25 ± 0.21\n        \n","output_type":"stream"},{"name":"stderr","text":"                                                                                ","output_type":"stream"},{"name":"stdout","text":"Epoch 6, Loss: 0.0901 \n LR: 0.0001\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 55.66% (241 / 433)\n\n                EER: 0.1196707\n                Threshold: -1.0581\n                Genuine distances: 0.78 ± 0.22\n                Impostor distances: 1.31 ± 0.20\n        \nEER: 0.1195 at threshold: 1.0574\n","output_type":"stream"},{"name":"stderr","text":"                                                                                ","output_type":"stream"},{"name":"stdout","text":"Epoch 7, Loss: 0.0796 \n LR: 0.0001\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 62.12% (269 / 433)\n\n                EER: 0.0960381\n                Threshold: -1.0435\n                Genuine distances: 0.75 ± 0.22\n                Impostor distances: 1.33 ± 0.21\n        \n","output_type":"stream"},{"name":"stderr","text":"                                                                                ","output_type":"stream"},{"name":"stdout","text":"Epoch 8, Loss: 0.0644 \n LR: 0.0001\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 65.36% (283 / 433)\n\n                EER: 0.1127837\n                Threshold: -1.0083\n                Genuine distances: 0.70 ± 0.24\n                Impostor distances: 1.29 ± 0.22\n        \n","output_type":"stream"},{"name":"stderr","text":"                                                                                ","output_type":"stream"},{"name":"stdout","text":"Epoch 9, Loss: 0.0653 \n LR: 0.0001\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 67.44% (292 / 433)\n\n                EER: 0.0989881\n                Threshold: -1.0581\n                Genuine distances: 0.72 ± 0.24\n                Impostor distances: 1.33 ± 0.21\n        \n","output_type":"stream"},{"name":"stderr","text":"                                                                                 ","output_type":"stream"},{"name":"stdout","text":"Epoch 10, Loss: 0.0528 \n LR: 0.0001\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 65.82% (285 / 433)\n\n                EER: 0.0976432\n                Threshold: -1.0386\n                Genuine distances: 0.70 ± 0.25\n                Impostor distances: 1.33 ± 0.21\n        \n","output_type":"stream"},{"name":"stderr","text":"                                                                                 ","output_type":"stream"},{"name":"stdout","text":"Epoch 11, Loss: 0.0431 \n LR: 0.0001\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 67.67% (293 / 433)\n\n                EER: 0.0866566\n                Threshold: -1.0233\n                Genuine distances: 0.67 ± 0.23\n                Impostor distances: 1.32 ± 0.21\n        \nEER: 0.0865 at threshold: 1.0224\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 ","output_type":"stream"},{"name":"stdout","text":"Epoch 12, Loss: 0.0458 \n LR: 0.0001\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 68.82% (298 / 433)\n\n                EER: 0.0956910\n                Threshold: -1.0004\n                Genuine distances: 0.67 ± 0.24\n                Impostor distances: 1.29 ± 0.21\n        \n","output_type":"stream"},{"name":"stderr","text":"                                                                                 ","output_type":"stream"},{"name":"stdout","text":"Epoch 13, Loss: 0.0358 \n LR: 0.0001\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 70.21% (304 / 433)\n\n                EER: 0.0692277\n                Threshold: -1.0141\n                Genuine distances: 0.65 ± 0.23\n                Impostor distances: 1.35 ± 0.21\n        \n","output_type":"stream"},{"name":"stderr","text":"                                                                                 ","output_type":"stream"},{"name":"stdout","text":"Epoch 14, Loss: 0.0328 \n LR: 0.0001\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 69.75% (302 / 433)\n\n                EER: 0.0775463\n                Threshold: -1.0093\n                Genuine distances: 0.65 ± 0.22\n                Impostor distances: 1.33 ± 0.21\n        \n","output_type":"stream"},{"name":"stderr","text":"                                                                                 ","output_type":"stream"},{"name":"stdout","text":"Epoch 15, Loss: 0.0281 \n LR: 0.0001\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 72.75% (315 / 433)\n\n                EER: 0.0695314\n                Threshold: -1.0069\n                Genuine distances: 0.63 ± 0.22\n                Impostor distances: 1.35 ± 0.22\n        \n","output_type":"stream"},{"name":"stderr","text":"                                                                                 ","output_type":"stream"},{"name":"stdout","text":"Epoch 16, Loss: 0.0260 \n LR: 0.0001\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 74.13% (321 / 433)\n\n                EER: 0.0737503\n                Threshold: -1.0452\n                Genuine distances: 0.66 ± 0.23\n                Impostor distances: 1.36 ± 0.20\n        \nEER: 0.0732 at threshold: 1.0455\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 ","output_type":"stream"},{"name":"stdout","text":"Epoch 17, Loss: 0.0272 \n LR: 0.0001\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 74.83% (324 / 433)\n\n                EER: 0.0673405\n                Threshold: -1.0497\n                Genuine distances: 0.66 ± 0.24\n                Impostor distances: 1.38 ± 0.20\n        \n","output_type":"stream"},{"name":"stderr","text":"                                                                                 ","output_type":"stream"},{"name":"stdout","text":"Epoch 18, Loss: 0.0244 \n LR: 0.0001\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 78.52% (340 / 433)\n\n                EER: 0.0646725\n                Threshold: -1.0230\n                Genuine distances: 0.63 ± 0.23\n                Impostor distances: 1.37 ± 0.20\n        \n","output_type":"stream"},{"name":"stderr","text":"                                                                                 ","output_type":"stream"},{"name":"stdout","text":"Epoch 19, Loss: 0.0223 \n LR: 0.0001\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 75.06% (325 / 433)\n\n                EER: 0.0667874\n                Threshold: -1.0237\n                Genuine distances: 0.63 ± 0.23\n                Impostor distances: 1.35 ± 0.20\n        \n","output_type":"stream"},{"name":"stderr","text":"                                                                                 ","output_type":"stream"},{"name":"stdout","text":"Epoch 20, Loss: 0.0225 \n LR: 0.0001\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 77.60% (336 / 433)\n\n                EER: 0.0685878\n                Threshold: -1.0453\n                Genuine distances: 0.64 ± 0.24\n                Impostor distances: 1.35 ± 0.19\n        \n","output_type":"stream"},{"name":"stderr","text":"                                                                                 ","output_type":"stream"},{"name":"stdout","text":"Epoch 21, Loss: 0.0237 \n LR: 0.0001\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 76.44% (331 / 433)\n\n                EER: 0.0743468\n                Threshold: -1.0300\n                Genuine distances: 0.63 ± 0.24\n                Impostor distances: 1.34 ± 0.20\n        \nEER: 0.0746 at threshold: 1.0306\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 ","output_type":"stream"},{"name":"stdout","text":"Epoch 22, Loss: 0.0174 \n LR: 0.0001\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 78.75% (341 / 433)\n\n                EER: 0.0665922\n                Threshold: -1.0317\n                Genuine distances: 0.64 ± 0.23\n                Impostor distances: 1.35 ± 0.20\n        \n","output_type":"stream"},{"name":"stderr","text":"                                                                                 ","output_type":"stream"},{"name":"stdout","text":"Epoch 23, Loss: 0.0154 \n LR: 0.0001\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 77.83% (337 / 433)\n\n                EER: 0.0623516\n                Threshold: -1.0017\n                Genuine distances: 0.62 ± 0.22\n                Impostor distances: 1.34 ± 0.20\n        \n","output_type":"stream"},{"name":"stderr","text":"                                                                                 ","output_type":"stream"},{"name":"stdout","text":"Epoch 24, Loss: 0.0168 \n LR: 0.0001\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 78.29% (339 / 433)\n\n                EER: 0.0735334\n                Threshold: -1.0148\n                Genuine distances: 0.62 ± 0.24\n                Impostor distances: 1.34 ± 0.21\n        \n","output_type":"stream"},{"name":"stderr","text":"                                                                                 ","output_type":"stream"},{"name":"stdout","text":"Epoch 25, Loss: 0.0135 \n LR: 0.0001\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 79.45% (344 / 433)\n\n                EER: 0.0631975\n                Threshold: -1.0458\n                Genuine distances: 0.62 ± 0.24\n                Impostor distances: 1.38 ± 0.20\n        \n","output_type":"stream"},{"name":"stderr","text":"                                                                                 ","output_type":"stream"},{"name":"stdout","text":"Epoch 26, Loss: 0.0130 \n LR: 0.0001\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 80.60% (349 / 433)\n\n                EER: 0.0555188\n                Threshold: -1.0600\n                Genuine distances: 0.64 ± 0.24\n                Impostor distances: 1.39 ± 0.19\n        \nEER: 0.0556 at threshold: 1.0595\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 ","output_type":"stream"},{"name":"stdout","text":"Epoch 27, Loss: 0.0122 \n LR: 0.0001\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 77.83% (337 / 433)\n\n                EER: 0.0665813\n                Threshold: -1.0428\n                Genuine distances: 0.63 ± 0.24\n                Impostor distances: 1.36 ± 0.20\n        \n","output_type":"stream"},{"name":"stderr","text":"                                                                                 ","output_type":"stream"},{"name":"stdout","text":"Epoch 28, Loss: 0.0085 \n LR: 0.0001\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 80.60% (349 / 433)\n\n                EER: 0.0518096\n                Threshold: -1.0206\n                Genuine distances: 0.62 ± 0.22\n                Impostor distances: 1.37 ± 0.19\n        \n","output_type":"stream"},{"name":"stderr","text":"                                                                                 ","output_type":"stream"},{"name":"stdout","text":"Epoch 29, Loss: 0.0096 \n LR: 0.0001\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 78.75% (341 / 433)\n\n                EER: 0.0594991\n                Threshold: -1.0414\n                Genuine distances: 0.63 ± 0.23\n                Impostor distances: 1.37 ± 0.19\n        \n","output_type":"stream"},{"name":"stderr","text":"                                                                                 ","output_type":"stream"},{"name":"stdout","text":"Epoch 30, Loss: 0.0076 \n LR: 0.0001\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 81.99% (355 / 433)\n\n                EER: 0.0578506\n                Threshold: -1.0389\n                Genuine distances: 0.63 ± 0.23\n                Impostor distances: 1.36 ± 0.19\n        \n","output_type":"stream"},{"name":"stderr","text":"                                                                                 ","output_type":"stream"},{"name":"stdout","text":"Epoch 31, Loss: 0.0096 \n LR: 0.0001\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 80.83% (350 / 433)\n\n                EER: 0.0555188\n                Threshold: -1.0236\n                Genuine distances: 0.62 ± 0.23\n                Impostor distances: 1.37 ± 0.20\n        \nEER: 0.0557 at threshold: 1.0235\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 ","output_type":"stream"},{"name":"stdout","text":"Epoch 32, Loss: 0.0083 \n LR: 0.0001\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 79.21% (343 / 433)\n\n                EER: 0.0624058\n                Threshold: -1.0152\n                Genuine distances: 0.61 ± 0.24\n                Impostor distances: 1.34 ± 0.19\n        \n","output_type":"stream"},{"name":"stderr","text":"                                                                                 ","output_type":"stream"},{"name":"stdout","text":"Epoch 33, Loss: 0.0056 \n LR: 0.0001\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 80.14% (347 / 433)\n\n                EER: 0.0670477\n                Threshold: -1.0492\n                Genuine distances: 0.62 ± 0.26\n                Impostor distances: 1.36 ± 0.20\n        \n","output_type":"stream"},{"name":"stderr","text":"                                                                                 ","output_type":"stream"},{"name":"stdout","text":"Epoch 34, Loss: 0.0076 \n LR: 0.0001\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 82.45% (357 / 433)\n\n                EER: 0.0618093\n                Threshold: -1.0323\n                Genuine distances: 0.62 ± 0.24\n                Impostor distances: 1.36 ± 0.19\n        \n","output_type":"stream"},{"name":"stderr","text":"                                                                                 ","output_type":"stream"},{"name":"stdout","text":"Epoch 35, Loss: 0.0047 \n LR: 0.0001\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 82.68% (358 / 433)\n\n                EER: 0.0496079\n                Threshold: -1.0164\n                Genuine distances: 0.61 ± 0.23\n                Impostor distances: 1.36 ± 0.19\n        \n","output_type":"stream"},{"name":"stderr","text":"                                                                                 ","output_type":"stream"},{"name":"stdout","text":"Epoch 36, Loss: 0.0061 \n LR: 0.0001\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 79.68% (345 / 433)\n\n                EER: 0.0598462\n                Threshold: -1.0314\n                Genuine distances: 0.62 ± 0.23\n                Impostor distances: 1.36 ± 0.19\n        \nEER: 0.0600 at threshold: 1.0314\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 ","output_type":"stream"},{"name":"stdout","text":"Epoch 37, Loss: 0.0062 \n LR: 0.0001\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 82.45% (357 / 433)\n\n                EER: 0.0666356\n                Threshold: -1.0139\n                Genuine distances: 0.61 ± 0.24\n                Impostor distances: 1.33 ± 0.19\n        \n","output_type":"stream"},{"name":"stderr","text":"                                                                                 ","output_type":"stream"},{"name":"stdout","text":"Epoch 38, Loss: 0.0062 \n LR: 0.0001\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 80.83% (350 / 433)\n\n                EER: 0.0549223\n                Threshold: -1.0422\n                Genuine distances: 0.63 ± 0.24\n                Impostor distances: 1.37 ± 0.18\n        \n","output_type":"stream"},{"name":"stderr","text":"                                                                                 ","output_type":"stream"},{"name":"stdout","text":"Epoch 39, Loss: 0.0057 \n LR: 1e-05\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 82.91% (359 / 433)\n\n                EER: 0.0554212\n                Threshold: -1.0516\n                Genuine distances: 0.63 ± 0.23\n                Impostor distances: 1.37 ± 0.18\n        \n","output_type":"stream"},{"name":"stderr","text":"                                                                                 ","output_type":"stream"},{"name":"stdout","text":"Epoch 40, Loss: 0.0041 \n LR: 1e-05\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 81.52% (353 / 433)\n\n                EER: 0.0596727\n                Threshold: -1.0611\n                Genuine distances: 0.64 ± 0.24\n                Impostor distances: 1.37 ± 0.18\n        \n","output_type":"stream"},{"name":"stderr","text":"                                                                                 ","output_type":"stream"},{"name":"stdout","text":"Epoch 41, Loss: 0.0025 \n LR: 1e-05\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 82.45% (357 / 433)\n\n                EER: 0.0568528\n                Threshold: -1.0534\n                Genuine distances: 0.63 ± 0.23\n                Impostor distances: 1.37 ± 0.18\n        \nEER: 0.0570 at threshold: 1.0547\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 ","output_type":"stream"},{"name":"stdout","text":"Epoch 42, Loss: 0.0039 \n LR: 1e-05\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 81.76% (354 / 433)\n\n                EER: 0.0574385\n                Threshold: -1.0515\n                Genuine distances: 0.63 ± 0.24\n                Impostor distances: 1.37 ± 0.18\n        \n","output_type":"stream"},{"name":"stderr","text":"                                                                                 ","output_type":"stream"},{"name":"stdout","text":"Epoch 43, Loss: 0.0044 \n LR: 1e-05\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 83.37% (361 / 433)\n\n                EER: 0.0531328\n                Threshold: -1.0491\n                Genuine distances: 0.63 ± 0.23\n                Impostor distances: 1.38 ± 0.18\n        \n","output_type":"stream"},{"name":"stderr","text":"                                                                                 ","output_type":"stream"},{"name":"stdout","text":"Epoch 44, Loss: 0.0025 \n LR: 1e-05\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 82.45% (357 / 433)\n\n                EER: 0.0562129\n                Threshold: -1.0456\n                Genuine distances: 0.63 ± 0.23\n                Impostor distances: 1.37 ± 0.18\n        \n","output_type":"stream"},{"name":"stderr","text":"                                                                                 ","output_type":"stream"},{"name":"stdout","text":"Epoch 45, Loss: 0.0031 \n LR: 1e-05\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 82.22% (356 / 433)\n\n                EER: 0.0574927\n                Threshold: -1.0559\n                Genuine distances: 0.63 ± 0.23\n                Impostor distances: 1.37 ± 0.18\n        \n","output_type":"stream"},{"name":"stderr","text":"                                                                                 ","output_type":"stream"},{"name":"stdout","text":"Epoch 46, Loss: 0.0012 \n LR: 1e-05\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 81.99% (355 / 433)\n\n                EER: 0.0551067\n                Threshold: -1.0548\n                Genuine distances: 0.63 ± 0.23\n                Impostor distances: 1.37 ± 0.18\n        \nEER: 0.0553 at threshold: 1.0550\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 ","output_type":"stream"},{"name":"stdout","text":"Epoch 47, Loss: 0.0025 \n LR: 1e-05\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 82.45% (357 / 433)\n\n                EER: 0.0545427\n                Threshold: -1.0480\n                Genuine distances: 0.63 ± 0.23\n                Impostor distances: 1.37 ± 0.18\n        \n","output_type":"stream"},{"name":"stderr","text":"                                                                                 ","output_type":"stream"},{"name":"stdout","text":"Epoch 48, Loss: 0.0018 \n LR: 1e-05\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 82.91% (359 / 433)\n\n                EER: 0.0513866\n                Threshold: -1.0447\n                Genuine distances: 0.62 ± 0.23\n                Impostor distances: 1.38 ± 0.18\n        \n","output_type":"stream"},{"name":"stderr","text":"                                                                                 ","output_type":"stream"},{"name":"stdout","text":"Epoch 49, Loss: 0.0032 \n LR: 1e-05\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 80.83% (350 / 433)\n\n                EER: 0.0569070\n                Threshold: -1.0566\n                Genuine distances: 0.63 ± 0.24\n                Impostor distances: 1.38 ± 0.18\n        \n","output_type":"stream"},{"name":"stderr","text":"                                                                                 ","output_type":"stream"},{"name":"stdout","text":"Epoch 50, Loss: 0.0021 \n LR: 1.0000000000000002e-06\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Accuracy = 81.76% (354 / 433)\n\n                EER: 0.0548464\n                Threshold: -1.0528\n                Genuine distances: 0.62 ± 0.23\n                Impostor distances: 1.38 ± 0.18\n        \n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"sns.set_theme(style=\"whitegrid\", context=\"talk\", palette=\"colorblind\")\nplt.rcParams['font.family'] = 'DejaVu Sans'\n\n        # 1. Loss train\nplt.figure(figsize=(13, 6))\nsns.lineplot(\n            x=range(len(train_losses)),\n            y=train_losses,\n            #label='Train Loss',\n            linewidth=2.5,\n            marker='o',\n            markersize=8\n)\n\n\nplt.title('Training Loss', fontsize=16, pad=20)\nplt.xlabel('Epoch', fontsize=14)\nplt.ylabel('Loss', fontsize=14)\n#plt.legend(title='', frameon=True, facecolor='white')\nplt.tight_layout()\nplt.suptitle(SUBTITLE)\n\nplt.savefig(f\"./loss_output.svg\", dpi=300, format=\"svg\", bbox_inches='tight')\nplt.close()\n\n# 2. Accuracy\nplt.figure(figsize=(13, 6))\npalette = sns.color_palette(\"husl\", 2)\n\neer_list2 = eer_list\nsns.lineplot(\n            x=range(len(eer_list)),\n            y=eer_list,\n            color=palette[0],\n            linewidth=2.5,\n            marker='^',\n            markersize=8\n)\n\nplt.title('ERR', fontsize=16, pad=20)\nplt.xlabel('Epoch', fontsize=14)\nplt.ylabel('ERR', fontsize=14)\n#plt.legend(title='', frameon=True, facecolor='white')\nplt.tight_layout()\nplt.suptitle(SUBTITLE)\nplt.savefig(f\"./eer_output.svg\", dpi=300, format=\"svg\", bbox_inches='tight')\nplt.close()\n\n\n# 2. Accuracy\nplt.figure(figsize=(13, 6))\npalette = sns.color_palette(\"husl\", 2)\n\nval_accur2 = val_accuracies\nsns.lineplot(\n            x=range(len(val_accuracies)),\n            y=[100 * i for i in val_accuracies],\n            #label='TOP 1 ACC',\n            color=palette[0],\n            linewidth=2.5,\n            marker='^',\n            markersize=8\n)\n\nplt.title('Validation Accuracy', fontsize=16, pad=20)\nplt.xlabel('Epoch', fontsize=14)\nplt.ylabel('Accuracy (%)', fontsize=14)\n#plt.legend(title='', frameon=True, facecolor='white')\nplt.tight_layout()\nplt.suptitle(SUBTITLE)\nplt.savefig(f\"./accuracy_output.svg\", dpi=300, format=\"svg\", bbox_inches='tight')\nplt.close()\nnp.savez(f'./array_losses.npz',\n                val_accuracies=val_accuracies)\n\n#np.savez(f'{self.dir_plot}/array_accuracy.npz',\n#                val_accuracies_top1=self.val_accuracies_top1,\n#                val_accuracies_top5=self.val_accuracies_top5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T17:41:59.817951Z","iopub.execute_input":"2025-05-12T17:41:59.818279Z","iopub.status.idle":"2025-05-12T17:42:00.633236Z","shell.execute_reply.started":"2025-05-12T17:41:59.818257Z","shell.execute_reply":"2025-05-12T17:42:00.632417Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.11/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.11/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.11/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.11/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.11/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n","output_type":"stream"}],"execution_count":51}]}